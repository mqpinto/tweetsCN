{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from langdetect import detect\n",
    "from textblob import TextBlob as tb\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_data = read_csv(\"dataset.csv\", encoding = \"ISO-8859-1\", sep=';', low_memory=False)\n",
    "\n",
    "# Select just some of the columns data\n",
    "nn_data = raw_data.loc[:, ['TextPost','Date']]\n",
    "INIT_TOTAL = len(nn_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all non english tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = []\n",
    "\n",
    "for text in nn_data['TextPost']:\n",
    "    try:\n",
    "        language = detect(text or '')\n",
    "        languages.append(language)\n",
    "    except:\n",
    "        languages.append('unknown')\n",
    "        \n",
    "nn_data['Language'] = languages\n",
    "nn_data = nn_data.loc[nn_data['Language'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add real Index column to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_data.insert(0, 'Index', range(0, len(nn_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Date column into date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DatetimeIndex(nn_data['Date'])\n",
    "del nn_data['Date']\n",
    "nn_data['Date'] = temp.date\n",
    "nn_data['Time'] = temp.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define transform_raw_text to make all transformations to tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_raw_text(text):\n",
    "    text = re.sub(r'\\B\\#[^\\s]+\\b\\s*', '', text) # Remove all hashtags\n",
    "    text = re.sub(r'\\B\\@[^\\s]+\\b\\s*', '', text) # Remove all people identifications\n",
    "    text = ' '.join(nltk.word_tokenize(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "polarities_sum = []\n",
    "polarities_avg = []\n",
    "total_sentences = []\n",
    "#languages = []\n",
    "polarities = []\n",
    "\n",
    "for text in nn_data['TextPost']:\n",
    "    #language = detect(text)\n",
    "    #languages.append(language)\n",
    "    text = transform_raw_text(text)\n",
    "    blob = tb(text)\n",
    "    sentences_polarities = list(map((lambda x: x.sentiment.polarity), blob.sentences))\n",
    "    p_sum = np.sum(sentences_polarities)\n",
    "    polarities_sum.append(p_sum)\n",
    "    polarities_avg.append(p_sum/len(sentences_polarities))\n",
    "    total_sentences.append(len(sentences_polarities))\n",
    "    polarities.append(''.join(str(x) + '|' for x in sentences_polarities))\n",
    "\n",
    "#nn_data['Language'] = languages\n",
    "nn_data['Sentences'] = total_sentences\n",
    "nn_data['Polarities'] = polarities\n",
    "nn_data['PolaritySum'] = polarities_sum\n",
    "nn_data['PolarityAvg'] = polarities_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show main results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rows = len(nn_data.loc[nn_data['PolarityAvg'] < 0])\n",
    "positive_rows = len(nn_data.loc[nn_data['PolarityAvg'] > 0])\n",
    "total_rows = len(nn_data)\n",
    "neutral_rows = total_rows - neg_rows - positive_rows\n",
    "\n",
    "print('Initital Total: {0}; Total EN: {1}; Neutral: {2}; Positive: {3}; Negative: {4}'.format(str(INIT_TOTAL),str(total_rows),str(neutral_rows),str(positive_rows),str(neg_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export dataset to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_data.to_csv('first_evaluation_results.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
